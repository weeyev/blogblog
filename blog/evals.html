<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Evaluating Your Evaluators - weeye</title>

    <meta
      name="description"
      content="An experiment using agentic reinforcement training to evaluate judge models and their biases in code generation."
    />
    <meta property="og:title" content="Evaluating Your Evaluators - weeye" />
    <meta
      property="og:description"
      content="An experiment using agentic reinforcement training to evaluate judge models and their biases in code generation"
    />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />

    <link rel="stylesheet" href="/tufte.css" />
    <link rel="stylesheet" href="/custom.css" />
    <!-- Prism.js for syntax highlighting -->
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css"
      rel="stylesheet"
      media="(prefers-color-scheme: dark)"
    />
    <style>
      article {
        padding: 1rem 0rem !important;
      }
      @media (prefers-color-scheme: dark) {
        .warning-box {
          background-color: transparent;
          border-left-color: #ff6b6b;
        }
        blockquote {
          background: transparent !important;
        }
        a {
          background: transparent !important;
        }
      }
      .warning-box {
        border-left: 4px solid #ff6b6b;
        background-color: transparent;
        padding: 1rem;
        margin: 1.5rem 0;
        font-size: 1.3rem;
        line-height: 2rem;
      }
      .disclaimer-box {
        background-color: #2a2a2a;
        border-left: 4px solid #ff6b6b;
        padding: 1.5rem;
        margin: 2rem 0;
        border-radius: 4px;
        color: #fff;
      }
      .disclaimer-box h3 {
        margin-top: 0;
        margin-bottom: 1rem;
        font-size: 1.2rem;
        font-weight: bold;
        color: #fff;
        letter-spacing: 0.1em;
      }
      .disclaimer-box p {
        margin: 0.8rem 0;
        font-size: 1rem;
        line-height: 1.5;
        color: #ddd;
      }
      blockquote {
        background-color: #2a2a2a !important;
        border-left: 4px solid #ff6b6b !important;
        padding: 2rem !important;
        margin: 3rem 0 !important;
        border-radius: 4px !important;
        color: #fff !important;
        width: 100% !important;
        box-sizing: border-box !important;
      }
      blockquote p {
        color: #ddd !important;
        width: 100% !important;
        margin-right: 0 !important;
        font-size: 1.4rem !important;
        line-height: 2rem !important;
        margin-top: 0 !important;
        margin-bottom: 0 !important;
        padding-right: 0 !important;
      }
      /* Make model names in results section bolder and bigger */
      h4 {
        font-size: 2rem !important;
        font-weight: bold !important;
        font-style: normal !important;
      }
      .links-section {
        margin-top: 0.5rem;
      }
      .links-section h3 {
        margin-bottom: 0.5rem;
      }
      .links-section ul {
        list-style-type: disc;
        margin-left: 1.5rem;
      }
      .links-section li {
        margin-bottom: 0.5rem;
      }
      
      .bookmark-item {
        margin-bottom: 2rem;
        padding: 1rem 0;
        border-bottom: 1px solid #333;
      }
      
      .bookmark-item:last-child {
        border-bottom: none;
      }
      
      .bookmark-item h3 {
        margin: 0 0 0.5rem 0;
        font-size: 1.3rem;
        font-weight: normal;
      }
      
      .bookmark-item h3 a {
        text-decoration: underline;
        color: #d4af37;
      }
      
      .bookmark-item .date {
        color: #888;
        font-size: 0.9rem;
        margin: 0.25rem 0;
        font-style: italic;
      }
      
      .bookmark-item p {
        margin: 0.5rem 0;
        line-height: 1.5;
      }
      
      .bookmark-item .tags {
        color: #d4af37;
        font-size: 0.9rem;
        font-family: monospace;
        margin-top: 0.75rem;
      }
      
      /* Image zoom functionality */
      .zoomable-image {
        cursor: pointer;
        transition: transform 0.2s ease;
      }
      
      .zoomable-image:hover {
        transform: scale(1.02);
      }
      
      .image-modal {
        display: none;
        position: fixed;
        z-index: 1000;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        background-color: rgba(0, 0, 0, 0.9);
        backdrop-filter: blur(5px);
      }
      
      .modal-content {
        margin: auto;
        display: block;
        width: 90%;
        max-width: 1200px;
        max-height: 90%;
        object-fit: contain;
        animation: zoom 0.3s ease;
      }
      
      @keyframes zoom {
        from { transform: scale(0.1); }
        to { transform: scale(1); }
      }
      
      .close {
        position: absolute;
        top: 15px;
        right: 35px;
        color: #f1f1f1;
        font-size: 40px;
        font-weight: bold;
        transition: 0.3s;
        cursor: pointer;
        user-select: none;
      }
      
      .close:hover,
      .close:focus {
        color: #bbb;
        text-decoration: none;
      }
      
      .modal-caption {
        margin: auto;
        display: block;
        width: 80%;
        max-width: 700px;
        text-align: center;
        color: #ccc;
        padding: 10px 0;
        height: 150px;
      }
    </style>
  </head>

  <body>
    <main>
      <header>
        <h1><a href="/index.html">[weeye]</a></h1>
        <nav>
          <ul>
            <li><a href="/projects.html">projects</a></li>
            <li><a href="/blogs.html">blogs</a></li>
            <li><a href="https://space.weeye.foo/">thoughts</a></li>
            <li><a href="/list.html">list</a></li>
          </ul>
        </nav>
        <hr />
      </header>

      <article>
        <header>
          <h1>Evaluating Your Evaluators</h1>
          <div class="blog-meta">September 7, 2025 â€¢ 12 min read</div>
        </header>

        <blockquote>
          <p>
            From "never wasting a motivated moment" - recently I read about the concept in a blog by <a href="https://twitter.com/arb8020" target="_blank" rel="noopener">@arb</a> on twt [<a href="https://arb8020.github.io/posts/motivated-moment/" target="_blank" rel="noopener">link</a>] and it made me wonder if a similar event led to me tinkering around and eventually writing this blog, even though at times it gets hard to consciously stick to the idea but recently I have tried to work on fixing this with keeping the side project pipeline as free as possible to at least work on something out of the current heap of ideas instead of doing nothing with them - hence expect more of these random experiments (and some serious ones too) in the near future, moving onto the thoughts/results from the experiments on autoRL.
          </p>
        </blockquote>

        <section>
          <h2>Setup</h2>
          <p>
            The purpose of this experiment is to use the agentic reinforcement training framework for evaluating how different judge models:
          </p>
          <ol>
            <li>Score the generated code on different parameters (e.g., accuracy, security, completeness, complexity, among others)</li>
            <li>The reasons for their strictness/leniency towards a specific model</li>
            <li>Whether a bias exists in judging out the code that was generated by itself (could adopt a process of randomizing the outputs and then evaluating them)</li>
          </ol>
          
          <p>
            Even though there were many multi-step setups on which this could have been tested out and would have been probably better than the current task of code generation, it just seemed as a fun thing to optimize and play around with than any other ones.
          </p>
          
          <p>
            The temperature is set to 1 for all the runs to be equally creative while generating their outputs and having a sense of variation in all the responses, without getting too deterministic in its approaches and limiting our evaluation for the same.
          </p>
          
          <p>
            Another point was about the choice of judges that was done with keeping the thought of including the SOTA and latest models from the big labs and some from the underdog ones while also not making me broke (in this case I was fortunate enough to have cool people come out [<a href="https://x.com/advith_krishnan" target="_blank" rel="noopener">goat</a>, <a href="https://x.com/MajorTimbWlf21" target="_blank" rel="noopener">goat</a>] to give free credits) with all the token calls from openrouter, the final judges are:
          </p>
          
          <div class="links-section">
            <div class="bookmark-item">
              <h3><a href="#" style="color: #d4af37;">Mistral 4.2-24b</a></h3>
              <p class="date">Added on September 7, 2025</p>
              <p>SOTA model from Mistral AI with strong reasoning capabilities and balanced performance across various tasks.</p>
              <p class="tags">ai/language_models mistral/reasoning</p>
            </div>
            
            <div class="bookmark-item">
              <h3><a href="#" style="color: #d4af37;">Gemma 3-27b-it</a></h3>
              <p class="date">Added on September 7, 2025</p>
              <p>Google's open-source model with instruction tuning, optimized for chat and instruction following tasks.</p>
              <p class="tags">ai/language_models google/open_source</p>
            </div>
            
            <div class="bookmark-item">
              <h3><a href="#" style="color: #d4af37;">Hermes 2-pro-llama-3-8b</a></h3>
              <p class="date">Added on September 7, 2025</p>
              <p>Enhanced version of Llama 3 with improved instruction following and reasoning capabilities.</p>
              <p class="tags">ai/language_models llama/enhanced</p>
            </div>
            
            <div class="bookmark-item">
              <h3><a href="#" style="color: #d4af37;">Deepseek v3-0324</a></h3>
              <p class="date">Added on September 7, 2025</p>
              <p>DeepSeek's latest model with strong coding and mathematical reasoning abilities.</p>
              <p class="tags">ai/language_models deepseek/coding</p>
            </div>
            
            <div class="bookmark-item">
              <h3><a href="#" style="color: #d4af37;">Kimi K-2</a></h3>
              <p class="date">Added on September 7, 2025</p>
              <p>Moonshot AI's model with extended context length and multimodal capabilities.</p>
              <p class="tags">ai/language_models moonshot/multimodal</p>
            </div>
            
            <div class="bookmark-item">
              <h3><a href="#" style="color: #d4af37;">Llama 3.3-70b-instruct</a></h3>
              <p class="date">Added on September 7, 2025</p>
              <p>Meta's large instruction-tuned model with excellent performance on complex reasoning tasks.</p>
              <p class="tags">ai/language_models meta/instruction_tuned</p>
            </div>
          </div>
          
          <p>
            Finally moving onto evaluating the performance, the benchmarking was something that seemed vague to figure out since there wasn't a direct method to do so, going from evaluating from the ruler scores for each epoch with a specific judge - finding its mean and comparing that to the base model with the same judge, to using the final weights of the trained model for manual A/B testing, or on any other open source platforms like deepeval, codejudge, to finally settling on the mean method (this could probably be optimized by building a framework of your own later on).
          </p>
        </section>

        <section>
          <h2>A Bit More Background on AutoRL and ART Framework</h2>
          <p>
            AutoRL â†’ RL without providing any labelled data for the GRPO to train on, instead evaluations are done by another LLM model, RULER generates its scores between 0 and 1, gives feedback and then the base model is trained again to improve its abilities.
          </p>
          
          <p>
            For all of these tasks we use ART (Agentic Reinforcement Trainer) by OpenPipe as a helper for RLAIF, the main goal here is to not find "excellent" or "well made" labelled datasets on which we can perform RL but to give a solid and structured prompt for the judge model.
          </p>
          
          <p>
            We start with the base model generating its set of training inputs, which are then saved for keeping every run fair and unbiased towards all the models (num_training_inputs=25), they are then divided into groups (num_inp_groups=2) ~ 12.5 groups - this helps in faster computation but also in giving some space/distinction between each problem to inherit some properties present. Now starting with group 1, the model generates 5 candidates each, which are then evaluated by the ruler and used to update the weights for the next epoch, even for a smaller training run of 10 epochs some of the models kind of learnt well to produce meaningful code in the end.
          </p>
          
          <p>The task description that was generated by GPT-4o is:</p>
          
          <div class="quantum-code">
            <pre><code>You are a professional programmer that can write quality code in various languages. You will be provided with the users text that would be a request for a programming task in natural language, Your goal is to generate a code solution to the programming task given, the code response should be structured in the following XML style tags:

&lt;request&gt;: This tag should wrap the user's original natural language request.

&lt;language&gt;: This tag should specify the programming language used in the code solution (e.g., Python, JavaScript, Java).

&lt;code&gt;: This tag should wrap the generated code block. The code should be well-commented.

For example, if the user's text is "write a python function that calculates the factorial of a number", the output should be:

&lt;request&gt;write a python function that calculates the factorial of a number&lt;/request&gt;
&lt;language&gt;Python&lt;/language&gt;
&lt;code&gt;
def factorial(n):
   if n &lt; 0:
       raise ValueError("Factorial is not defined for negative numbers.")
   elif n == 0 or n == 1:
       return 1
   else:
       result = 1
       for i in range(2, n + 1):
           result *= i
       return result
&lt;/code&gt;</code></pre>
          </div>
        </section>

        <section>
          <h2>Observations</h2>
          <p>
            Though condensing the results in a short format seems really unfair here but I will try to give a general outlook of how things turned out in the end, but just to give a brief idea of how the candidates were generated and the quality of code we will review a log file of a random trajectory with the Mistral 4.2-24b.
          </p>
          
          <p><strong>GENERATED TASK:</strong> Create a REST API endpoint in Node.js Express that handles user login with JWT token generation.</p>
          
          <p><strong>Trajectory-1:</strong> SCORE= 0.4 - RULER explanation: This is a very basic implementation. It gets the job done by providing a login endpoint and generating a JWT, but it uses hardcoded credentials which is a major security flaw and lacks any password hashing. It fulfills the core request but very poorly.</p>
          
          <div class="quantum-code">
            <pre><code class="language-javascript">const express = require('express');
const jwt = require('jsonwebtoken');

const app = express();
const port = process.env.PORT || 3000;

app.use(express.json());

app.post('/login', (req, res) => {
  const { username, password } = req.body;

  // Simulate user authentication
  if (username === 'admin' && password === 'password') {
    // Generate JWT token
    const token = jwt.sign({ userId: '123456', username }, 'secretKey', { expiresIn: '1h' });

    // Return token to client
    res.json({ token });
  } else {
    res.status(401).json({ message: 'Invalid credentials' });
  }
});

app.listen(port, () => console.log(`Server running on port ${port}`));</code></pre>
          </div>
          
          <p><strong>Trajectory 5:</strong> SCORE= 1.0 - RULER explanation: This trajectory provides the most complete solution. It includes registration and login, bcrypt for password hashing and verification, and a middleware function to protect routes ensuring a higher level of security. It even includes authentication with JWTs, and has error handling. This trajectory is closest to a production ready pattern.</p>
          
          <div class="quantum-code">
            <pre><code class="language-javascript">const express = require('express');
const jwt = require('jsonwebtoken');
const bcrypt = require('bcrypt');

const app = express();
const PORT = process.env.PORT || 3000;

// User data (replace with actual data)
const users = [
  { id: 1, username: 'user1', password: 'password1' },
  { id: 2, username: 'user2', password: 'password2' }
];

// Check if the user is logged in
const isLoggedIn = (req, res, next) => {
  const token = req.headers['authorization'].split(' ')[1];

  if (!token) {
    return res.status(401).send({ error: 'Unauthorized' });
  }

  jwt.verify(token, process.env.SECRET_KEY, (err, decoded) => {
    if (err || decoded.id !== req.user.id) {
      return res.status(401).send({ error: 'Unauthorized' });
    }

    req.user = decoded;
    next();
  });
};

// Register a new user
app.post('/register', (req, res) => {
  const { username, password } = req.body;

  const user = {
    id: users.length + 1,
    username
  };

  bcrypt.hash(password, 10).then(hash => {
    user.password = hash;
    users.push(user);
    res.status(201).send({ message: 'User registered', user: user });
  }).catch(err => {
    res.status(400).send({ error: 'Registration failed' });
  });
});

// Login a user
app.post('/login', (req, res) => {
  const { username, password } = req.body;

  users.forEach(user => {
    if (user.username === username && bcrypt.compareSync(password, user.password)) {
      const token = jwt.sign({ id: user.id, username }, process.env.SECRET_KEY, { expiresIn: '1h' });

      res.status(200).send({ token });
    }
  });

  res.status(401).send({ error: 'Invalid credentials' });
});

// Example routes
app.get('/protected', isLoggedIn, (req, res) => {
  res.send('Welcome to the protected area');
});

app.listen(PORT, () => console.log(`Server running on port ${PORT}`));</code></pre>
          </div>
          
          <p>
            Funnily enough, the ruler remarks this trajectory as closest to "a production-ready pattern" but forget about the different security issues that it has like going to the extent of adopting to store the password as a literal string, the generated code doesn't even execute with it having multiple bugs like not importing the middleware while using it, and leaving the SECRET_KEY variable undefined.
          </p>
          
          <p>
            A naive explanation for this might be how the judge model keeps checking for improvements in using specific keywords, complexity of the code, and better feature additions from the initial trajectory - without a sense of whether the solution even executes first, maybe ordering the parameters while evaluation could help with this issue, or using a model with reasoning mode would capture these better.
          </p>
        </section>

        <section>
          <h2>Results</h2>
          <p>
            Even though all the trajectories were supposed to be evaluated by the ruler but often in between the models generated some codes that were out of the structure of the JSON model for that specific judge to evaluate. Let's split the judges now in 3 categories on the basis of their performance.
          </p>
          
          <h3>The Mediocre Models</h3>
          
          <h4>Mistral 3.2-24b</h4>
          <p>
            Mistral performs fairly well with mean and median scores of 62.7 and 70, and is consistent with its score having relatively fewer spikes and deviations than the other models, leading to an assumption of it having a reliable learning signal.
          </p>
          <figure>
            <img src="../assets/imgs/ruler_eval_mistral.jpeg" alt="Mistral evaluation results" class="zoomable-image" style="width: 100%; height: auto; margin: 1rem 0;" />
            <figcaption>Mistral 3.2-24b evaluation results</figcaption>
          </figure>
          
          <h4>Gemma 3-27b-it</h4>
          <p>
            The mean score for Gemma came out to be 58.7 and median as 60, it had more erratic runs and often dipped quite highly with its evals, comparing the standard deviation of it (~0.308) with the Mistral one (~0.320) doesn't quite tell the whole story, until a simple data analysis prompt on any LLM shows you the IQR - surprisingly both have the same IQR of 0.50 but for Gemma (Q1=0.35, Q3=0.85) and for Mistral (Q1=0.40, 0.90) highlighting the upward shifted values/lesser drop in the scores.
          </p>
          <figure>
            <img src="../assets/imgs/ruler_eval_gemma.jpeg" alt="Gemma evaluation results" class="zoomable-image" style="width: 100%; height: auto; margin: 1rem 0;" />
            <figcaption>Gemma 3-27b-it evaluation results</figcaption>
          </figure>
          
          <h3>The Better Models</h3>
          
          <h4>Hermes 2-pro-llama-3-8b</h4>
          <p>
            This was surprisingly (or unsurprisingly for some of us) the best performer in the set, with a mean score of 75.4 and a median of 80.0, and above that, it had a standard deviation of ~0.209, making it the most consistent, also with it rarely dropping below 0.5 and an IQR of 0.250. Now like a normal human being who likes judging things by the "taste" of them over some statistical results, I used the final weights on newer problems that weren't baseline easy and it actually gave an executable and a pretty well-structured code, unlike the previous ones that had a pretty broken result.
          </p>
          <figure>
            <img src="../assets/imgs/ruler_eval_hermes.jpeg" alt="Hermes evaluation results" class="zoomable-image" style="width: 100%; height: auto; margin: 1rem 0;" />
            <figcaption>Hermes 2-pro-llama-3-8b evaluation results</figcaption>
          </figure>
          
          <h4>Deepseek v3-0324</h4>
          <p>
            One of the other models that I really like, at the time of writing this they have a newer extension v3.1 which has improved thinking capacities over its predecessors by a margin, expectedly so even v3 trailed Hermes with scores of 69.2 and 70 for mean and median, another really abnormal/unique thing that I saw after while surfing the CSV result was couple of trajectories had a score of 2.0, while the ruler is supposed to give scores between 0 and 1 the only plausible reason that I could speculate of was maybe the judge finding that particular trajectory exceptionally better than the other trajectories in the same group unless this was a bug or a messup on my part. (After clarification from the devs, it indeed seems like a bug)
          </p>
          <figure>
            <img src="../assets/imgs/ruler_eval_deepseek.jpeg" alt="Deepseek evaluation results" class="zoomable-image" style="width: 100%; height: auto; margin: 1rem 0;" />
            <figcaption>Deepseek v3-0324 evaluation results</figcaption>
            <img src="../assets/imgs/dms.png" alt="dms" class="zoomable-image" style="width: 100%; height: auto; margin: 0rem 0;" />

          </figure>
          
          <h3>The "Not" So Good Models</h3>
          
          <h4>Kimi K-2</h4>
          <p>
            Before starting this experiment, I was explicitly excited to try out Kimi and Hermes expecting them to top out the list, however unlike Hermes, Kimi doesn't come out as a numerical winner with mean and median of 38.2 and 25 putting it around the bottom instead with severe deviations, the graph looks more like an ECG display of a person having a heart attack, it made me wonder whether this was due to how the architecture of Kimi makes it inherently as more of a stricter judge than others, with around 1T parameters (~32 billion active per inference) and a total 384 experts, over that it has a post-training RL layer to optimize the model for verifiable rewards rather than subjectivity and creativity which models like Hermes prioritize more. While this might be an efficient parameter for judging but I did not want to limit the benchmarking to just "correct" or "incorrect" solutions, though another version of this just for correctness would be a fun thing to try out in future.
          </p>
          <figure>
            <img src="../assets/imgs/ruler_eval_kimi_dark.png" alt="Kimi evaluation results" class="zoomable-image" style="width: 100%; height: auto; margin: 1rem 0;" />
            <figcaption>Kimi K-2 evaluation results</figcaption>
          </figure>
          
          <h4>Llama 3.3-70b-instruct</h4>
          <p>
            Another case of relatively weird/unexpected results, while Llama does not score on the higher end but it definitely looks like most consistent model among all until you see that the drops are severe with it even going twice to zero, the standard deviation of 0.224 is still in the better range and except for these instances, you can rarely see a lot of variance. The mean and median for it are 58 and 60.0 with it never giving a perfect score but always being around the range of 0.95-0.98 something that is quite unique and makes you like Meta more.
          </p>
          <figure>
            <img src="../assets/imgs/ruler_eval_llama_dark.png" alt="Llama evaluation results" class="zoomable-image" style="width: 100%; height: auto; margin: 1rem 0;" />
            <figcaption>Llama 3.3-70b-instruct evaluation results</figcaption>
          </figure>
        </section>

        <section>
          <h2>The Bias Experiment</h2>
          <p>
            As mentioned earlier, the plan is to check whether a specific judge model is biased towards scoring a base model when we keep both of them the same or just vary different versions of the base.
          </p>
          
          <p>
            Initially on keeping the base and judge model same (namely Qwen 2.5 1.5b instruct) there was marginal improvement in the scores from the initial scores of 61.4 and 68 for mean and median, to ~62 and 69 eventually, indicating at least not a surface level bias (this has to be further extended to other models but due to my other commitments I'm too lazy to go back and verify it), when switching the judge to a further newer model qwen/qwen3-32b, again not a significant improvement is seen but still the scores bumped up in the range of ~0.5-1 for both, while saying that there has been a definite improvement/degradation seems a bit too far fetched and again needs to be checked with other sample models also.
          </p>
        </section>

        <section>
          <h2>Conclusions</h2>
          <p>
            This started out as a fun experiment to test out the OpenPipe ruler framework and AutoRL environment and in between turned serious for a while, where I had many ideas of doing this without ruler by adopting either making a whole environment from the ground up or using any other framework that provides better reasoning/thought communication for tackling the problem of ruler scores not achieving their real values due to the accumulation of either a lot of parameters to verify and thereby missing some or the other one or misalignment of the algorithm used with the outputs.
          </p>
          
          <p>
            Another suggestion that I received was testing the judges with a specific number of shots per each judge - like chain of thought shot (have a strong intuition that this would help increasing the scores), example shots, safety shots among others so we don't have to consider each parameter together and make the process more clumsier, reducing biases/variances if any and provide more thinking space.
          </p>
          
          <p>Thank you for reading this and for any feedback - dm or mail me</p>
      </article>

      <footer>
        <hr />
        <div class="footer-credit">
          <span>Â© 2025 [weeye]. All rights reserved.</span>
          <span aria-hidden="true">Â·</span>
          <span>
            Built with
            <a
              href="https://github.com/edwardtufte/tufte-css"
              target="_blank"
              rel="noopener"
              >Tufte CSS</a
            >
          </span>
        </div>
      </footer>
    </main>

    <!-- Image Modal -->
    <div id="imageModal" class="image-modal">
      <span class="close">&times;</span>
      <img class="modal-content" id="modalImage">
      <div class="modal-caption" id="modalCaption"></div>
    </div>

    <!-- Prism.js for syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <script>
      // Image zoom functionality
      document.addEventListener('DOMContentLoaded', function() {
        const modal = document.getElementById('imageModal');
        const modalImg = document.getElementById('modalImage');
        const modalCaption = document.getElementById('modalCaption');
        const closeBtn = document.getElementsByClassName('close')[0];
        const zoomableImages = document.getElementsByClassName('zoomable-image');

        // Add click event to all zoomable images
        for (let i = 0; i < zoomableImages.length; i++) {
          zoomableImages[i].addEventListener('click', function() {
            modal.style.display = 'block';
            modalImg.src = this.src;
            modalCaption.innerHTML = this.alt;
          });
        }

        // Close modal when clicking the X
        closeBtn.addEventListener('click', function() {
          modal.style.display = 'none';
        });

        // Close modal when clicking outside the image
        modal.addEventListener('click', function(event) {
          if (event.target === modal) {
            modal.style.display = 'none';
          }
        });

        // Close modal with Escape key
        document.addEventListener('keydown', function(event) {
          if (event.key === 'Escape' && modal.style.display === 'block') {
            modal.style.display = 'none';
          }
        });
      });
    </script>
  </body>
</html>
